@inproceedings{hou-etal-2023-cone,
 abstract = {This paper tackles an emerging and challenging problem of long video temporal grounding (VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13 to 6.87% on MAD) and state-of-the-art results. Analyses also reveal higher efficiency as the query-guided window selection mechanism accelerates inference time by 2x on Ego4D-NLQ and 15x on MAD while keeping SOTA results. Codes have been released at rÌ†lhttps://github.com/houzhijian/CONE.},
 address = {Toronto, Canada},
 author = {Hou, Zhijian  and
Zhong, Wanjun  and
Ji, Lei  and
Gao, Difei  and
Yan, Kun  and
Chan, W.k.  and
Ngo, Chong-Wah  and
Shou, Mike Zheng  and
Duan, Nan},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.445},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 month = {July},
 pages = {8013--8028},
 publisher = {Association for Computational Linguistics},
 title = {CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding},
 url = {https://aclanthology.org/2023.acl-long.445},
 year = {2023}
}
