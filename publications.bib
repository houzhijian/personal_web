@inproceedings{hou2021conquer,
  title={CONQUER: Contextual query-aware ranking for video corpus moment retrieval},
  author={Hou, Zhijian and Ngo, Chong-Wah},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={3900--3908},
  year={2021}
}

@inproceedings{hou-etal-2023-cone,
    title = "{CONE}: An Efficient {CO}arse-to-fi{NE} Alignment Framework for Long Video Temporal Grounding",
    author = "Hou, Zhijian  and
      Zhong, Wanjun  and
      Ji, Lei  and
      Gao, Difei  and
      Yan, Kun  and
      Chan, W.k.  and
      Ngo, Chong-Wah  and
      Shou, Mike Zheng  and
      Duan, Nan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.445",
    doi = "10.18653/v1/2023.acl-long.445",
    pages = "8013--8028",
    abstract = "This paper tackles an emerging and challenging problem of long video temporal grounding (VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13 to 6.87{\%} on MAD) and state-of-the-art results. Analyses also reveal higher efficiency as the query-guided window selection mechanism accelerates inference time by 2x on Ego4D-NLQ and 15x on MAD while keeping SOTA results. Codes have been released at \url{https://github.com/houzhijian/CONE}.",
}

@inproceedings{ma2022reinforcement,
  title={Reinforcement learning-based interactive video search},
  author={Ma, Zhixin and Wu, Jiaxin and Hou, Zhijian and Ngo, Chong-Wah},
  booktitle={International Conference on Multimedia Modeling},
  pages={549--555},
  year={2022},
  organization={Springer}
}

@article{wu2023likelihood,
  title={(Un) likelihood Training for Interpretable Embedding},
  author={Wu, Jiaxin and Ngo, Chong-Wah and Chan, Wing-Kwong and Hou, Zhijian},
  journal={ACM Transactions on Information Systems},
  volume={42},
  number={3},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY}
}

@article{hou2023groundnlq,
  title={Groundnlq@ ego4d natural language queries challenge 2023},
  author={Hou, Zhijian and Ji, Lei and Gao, Difei and Zhong, Wanjun and Yan, Kun and Li, Chao and Chan, Wing-Kwong and Ngo, Chong-Wah and Duan, Nan and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2306.15255},
  year={2023}
}

@article{hou2022efficient,
  title={An efficient coarse-to-fine alignment framework@ ego4d natural language queries challenge 2022},
  author={Hou, Zhijian and Zhong, Wanjun and Ji, Lei and Gao, Difei and Yan, Kun and Chan, Wing-Kwong and Ngo, Chong-Wah and Shou, Zheng and Duan, Nan},
  journal={arXiv preprint arXiv:2211.08776},
  year={2022}
}

